{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch>=1.4.0 in c:\\users\\chominkyung\\miniconda3\\envs\\jupyter\\lib\\site-packages (from -r requirements.txt (line 1)) (1.12.1)\n",
      "Requirement already satisfied: torchvision>=0.5.0 in c:\\users\\chominkyung\\miniconda3\\envs\\jupyter\\lib\\site-packages (from -r requirements.txt (line 2)) (0.13.1)\n",
      "Requirement already satisfied: dominate>=2.4.0 in c:\\users\\chominkyung\\miniconda3\\envs\\jupyter\\lib\\site-packages (from -r requirements.txt (line 3)) (2.7.0)\n",
      "Requirement already satisfied: visdom>=0.1.8.8 in c:\\users\\chominkyung\\miniconda3\\envs\\jupyter\\lib\\site-packages (from -r requirements.txt (line 4)) (0.2.4)\n",
      "Requirement already satisfied: wandb in c:\\users\\chominkyung\\miniconda3\\envs\\jupyter\\lib\\site-packages (from -r requirements.txt (line 5)) (0.15.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\chominkyung\\miniconda3\\envs\\jupyter\\lib\\site-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (4.3.0)\n",
      "Requirement already satisfied: requests in c:\\users\\chominkyung\\miniconda3\\envs\\jupyter\\lib\\site-packages (from torchvision>=0.5.0->-r requirements.txt (line 2)) (2.28.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\chominkyung\\miniconda3\\envs\\jupyter\\lib\\site-packages (from torchvision>=0.5.0->-r requirements.txt (line 2)) (1.23.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\chominkyung\\miniconda3\\envs\\jupyter\\lib\\site-packages (from torchvision>=0.5.0->-r requirements.txt (line 2)) (9.2.0)\n",
      "Requirement already satisfied: websocket-client in c:\\users\\chominkyung\\miniconda3\\envs\\jupyter\\lib\\site-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (1.4.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\chominkyung\\miniconda3\\envs\\jupyter\\lib\\site-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (3.1)\n",
      "Requirement already satisfied: six in c:\\users\\chominkyung\\miniconda3\\envs\\jupyter\\lib\\site-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (1.16.0)\n",
      "Requirement already satisfied: tornado in c:\\users\\chominkyung\\miniconda3\\envs\\jupyter\\lib\\site-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (6.2)\n",
      "Requirement already satisfied: jsonpatch in c:\\users\\chominkyung\\miniconda3\\envs\\jupyter\\lib\\site-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (1.32)\n",
      "Requirement already satisfied: scipy in c:\\users\\chominkyung\\miniconda3\\envs\\jupyter\\lib\\site-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (1.10.1)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\chominkyung\\miniconda3\\envs\\jupyter\\lib\\site-packages (from wandb->-r requirements.txt (line 5)) (5.9.2)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in c:\\users\\chominkyung\\miniconda3\\envs\\jupyter\\lib\\site-packages (from wandb->-r requirements.txt (line 5)) (1.22.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in c:\\users\\chominkyung\\miniconda3\\envs\\jupyter\\lib\\site-packages (from wandb->-r requirements.txt (line 5)) (4.22.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\chominkyung\\miniconda3\\envs\\jupyter\\lib\\site-packages (from wandb->-r requirements.txt (line 5)) (63.4.1)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in c:\\users\\chominkyung\\miniconda3\\envs\\jupyter\\lib\\site-packages (from wandb->-r requirements.txt (line 5)) (3.1.31)\n",
      "Requirement already satisfied: setproctitle in c:\\users\\chominkyung\\miniconda3\\envs\\jupyter\\lib\\site-packages (from wandb->-r requirements.txt (line 5)) (1.3.2)\n",
      "Requirement already satisfied: pathtools in c:\\users\\chominkyung\\miniconda3\\envs\\jupyter\\lib\\site-packages (from wandb->-r requirements.txt (line 5)) (0.1.2)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in c:\\users\\chominkyung\\miniconda3\\envs\\jupyter\\lib\\site-packages (from wandb->-r requirements.txt (line 5)) (8.1.3)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in c:\\users\\chominkyung\\miniconda3\\envs\\jupyter\\lib\\site-packages (from wandb->-r requirements.txt (line 5)) (1.4.4)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\chominkyung\\miniconda3\\envs\\jupyter\\lib\\site-packages (from wandb->-r requirements.txt (line 5)) (6.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\chominkyung\\miniconda3\\envs\\jupyter\\lib\\site-packages (from wandb->-r requirements.txt (line 5)) (0.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\chominkyung\\miniconda3\\envs\\jupyter\\lib\\site-packages (from Click!=8.0.0,>=7.0->wandb->-r requirements.txt (line 5)) (0.4.5)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\chominkyung\\miniconda3\\envs\\jupyter\\lib\\site-packages (from GitPython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 5)) (4.0.10)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\chominkyung\\miniconda3\\envs\\jupyter\\lib\\site-packages (from requests->torchvision>=0.5.0->-r requirements.txt (line 2)) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\chominkyung\\miniconda3\\envs\\jupyter\\lib\\site-packages (from requests->torchvision>=0.5.0->-r requirements.txt (line 2)) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\chominkyung\\miniconda3\\envs\\jupyter\\lib\\site-packages (from requests->torchvision>=0.5.0->-r requirements.txt (line 2)) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\chominkyung\\miniconda3\\envs\\jupyter\\lib\\site-packages (from requests->torchvision>=0.5.0->-r requirements.txt (line 2)) (1.26.12)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\chominkyung\\miniconda3\\envs\\jupyter\\lib\\site-packages (from jsonpatch->visdom>=0.1.8.8->-r requirements.txt (line 4)) (2.3)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\chominkyung\\miniconda3\\envs\\jupyter\\lib\\site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 5)) (5.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련 전 준비"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "server 키기 (시각화)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python -m visdom.server"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 전처리 : A와 B사진을 묶어 test, train, val에 담는다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chominkyung\\Documents\\GitHub\\capstone\\eyebrow_synthesis\n"
     ]
    }
   ],
   "source": [
    "!cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "why?????\n",
      "[fold_A] =  datasets/eyebrow_remove/A\n",
      "[fold_B] =  datasets/eyebrow_remove/B\n",
      "[fold_AB] =  datasets/eyebrow_remove\n",
      "[num_imgs] =  1000000\n",
      "[use_AB] =  False\n",
      "[no_multiprocessing] =  False\n",
      "split = test, use 30/30 images\n",
      "split = test, number of images = 30\n",
      "split = train, use 114/114 images\n",
      "split = train, number of images = 114\n",
      "split = val, use 44/44 images\n",
      "split = val, number of images = 44\n",
      "why?????\n",
      "why?????\n",
      "why?????\n",
      "why?????\n",
      "why?????\n",
      "why?????\n",
      "why?????\n",
      "why?????\n",
      "why?????\n",
      "why?????\n",
      "why?????\n",
      "why?????\n",
      "why?????\n",
      "why?????\n",
      "why?????\n",
      "why?????\n"
     ]
    }
   ],
   "source": [
    "!python datasets/combine_A_and_B.py --fold_A datasets/eyebrow_remove/A --fold_B datasets/eyebrow_remove/B --fold_AB datasets/eyebrow_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python datasets/combine_A_and_B.py --fold_A datasets/eyebrow_synthesis/A --fold_B datasets/eyebrow_synthesis/B --fold_AB datasets/eyebrow_synthesis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU 정리 : 터미널창에 nvidia-smi 치고 process kill 하는게 제일 효과적 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch 제대로 깔렸는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "print(cuda)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Options ---------------\n",
      "               batch_size: 1                             \n",
      "                    beta1: 0.5                           \n",
      "          checkpoints_dir: ./checkpoints                 \n",
      "           continue_train: False                         \n",
      "                crop_size: 256                           \n",
      "                 dataroot: ./datasets/eyebrow_synthesis  \t[default: None]\n",
      "             dataset_mode: aligned                       \n",
      "                direction: AtoB                          \n",
      "              display_env: main                          \n",
      "             display_freq: 400                           \n",
      "               display_id: 1                             \n",
      "            display_ncols: 4                             \n",
      "             display_port: 8097                          \n",
      "           display_server: http://localhost              \n",
      "          display_winsize: 256                           \n",
      "                    epoch: latest                        \n",
      "              epoch_count: 1                             \n",
      "                 gan_mode: vanilla                       \n",
      "                  gpu_ids: 0                             \n",
      "                init_gain: 0.02                          \n",
      "                init_type: normal                        \n",
      "                 input_nc: 3                             \n",
      "                  isTrain: True                          \t[default: None]\n",
      "                lambda_L1: 100.0                         \n",
      "                load_iter: 0                             \t[default: 0]\n",
      "                load_size: 286                           \n",
      "                       lr: 0.0002                        \n",
      "           lr_decay_iters: 50                            \n",
      "                lr_policy: linear                        \n",
      "         max_dataset_size: inf                           \n",
      "                    model: pix2pix                       \t[default: cycle_gan]\n",
      "                 n_epochs: 100                           \n",
      "           n_epochs_decay: 100                           \n",
      "               n_layers_D: 3                             \n",
      "                     name: eyebrow_synthesis             \t[default: experiment_name]\n",
      "                      ndf: 64                            \n",
      "                     netD: basic                         \n",
      "                     netG: unet_256                      \n",
      "                      ngf: 64                            \n",
      "               no_dropout: False                         \n",
      "                  no_flip: False                         \n",
      "                  no_html: False                         \n",
      "                     norm: batch                         \n",
      "              num_threads: 4                             \n",
      "                output_nc: 3                             \n",
      "                    phase: train                         \n",
      "                pool_size: 0                             \n",
      "               preprocess: resize_and_crop               \n",
      "               print_freq: 100                           \n",
      "             save_by_iter: False                         \n",
      "          save_epoch_freq: 5                             \n",
      "         save_latest_freq: 5000                          \n",
      "           serial_batches: False                         \n",
      "                   suffix:                               \n",
      "         update_html_freq: 1000                          \n",
      "                use_wandb: False                         \n",
      "                  verbose: False                         \n",
      "       wandb_project_name: CycleGAN-and-pix2pix          \n",
      "----------------- End -------------------\n",
      "this is base_options\n",
      "[0]\n",
      "dataset [AlignedDataset] was created\n",
      "The number of training images = 124\n",
      "initialize network with normal\n",
      "initialize network with normal\n",
      "model [Pix2PixModel] was created\n",
      "---------- Networks initialized -------------\n",
      "[Network G] Total number of parameters : 54.414 M\n",
      "[Network D] Total number of parameters : 2.769 M\n",
      "-----------------------------------------------\n",
      "create web directory ./checkpoints\\eyebrow_synthesis\\web...\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 1, iters: 100, time: 0.086, data: 3.039) G_GAN: 1.708 G_L1: 10.160 D_real: 0.365 D_fake: 0.256 \n",
      "End of epoch 1 / 200 \t Time Taken: 17 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 2, iters: 76, time: 0.088, data: 0.000) G_GAN: 0.771 G_L1: 5.755 D_real: 0.477 D_fake: 0.853 \n",
      "End of epoch 2 / 200 \t Time Taken: 11 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 3, iters: 52, time: 0.099, data: 0.000) G_GAN: 0.933 G_L1: 4.363 D_real: 0.815 D_fake: 0.581 \n",
      "End of epoch 3 / 200 \t Time Taken: 11 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 4, iters: 28, time: 0.395, data: 0.001) G_GAN: 0.850 G_L1: 3.454 D_real: 0.728 D_fake: 0.521 \n",
      "End of epoch 4 / 200 \t Time Taken: 11 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 5, iters: 4, time: 0.081, data: 0.001) G_GAN: 0.905 G_L1: 3.406 D_real: 0.553 D_fake: 0.611 \n",
      "(epoch: 5, iters: 104, time: 0.087, data: 0.000) G_GAN: 0.662 G_L1: 3.440 D_real: 1.024 D_fake: 0.315 \n",
      "saving the model at the end of epoch 5, iters 620\n",
      "End of epoch 5 / 200 \t Time Taken: 11 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 6, iters: 80, time: 0.083, data: 0.001) G_GAN: 0.927 G_L1: 2.950 D_real: 0.411 D_fake: 0.894 \n",
      "End of epoch 6 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 7, iters: 56, time: 0.251, data: 0.000) G_GAN: 0.927 G_L1: 3.340 D_real: 0.750 D_fake: 0.512 \n",
      "End of epoch 7 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 8, iters: 32, time: 0.082, data: 0.001) G_GAN: 0.779 G_L1: 3.288 D_real: 0.676 D_fake: 0.673 \n",
      "End of epoch 8 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 9, iters: 8, time: 0.082, data: 0.000) G_GAN: 1.050 G_L1: 2.541 D_real: 0.382 D_fake: 0.526 \n",
      "(epoch: 9, iters: 108, time: 0.085, data: 0.001) G_GAN: 0.856 G_L1: 3.262 D_real: 1.118 D_fake: 0.319 \n",
      "End of epoch 9 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 10, iters: 84, time: 0.269, data: 0.000) G_GAN: 0.592 G_L1: 2.864 D_real: 0.679 D_fake: 0.459 \n",
      "saving the model at the end of epoch 10, iters 1240\n",
      "End of epoch 10 / 200 \t Time Taken: 12 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 11, iters: 60, time: 0.100, data: 0.000) G_GAN: 0.863 G_L1: 2.418 D_real: 0.457 D_fake: 0.727 \n",
      "End of epoch 11 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 12, iters: 36, time: 0.096, data: 0.000) G_GAN: 0.936 G_L1: 2.617 D_real: 0.511 D_fake: 0.568 \n",
      "End of epoch 12 / 200 \t Time Taken: 11 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 13, iters: 12, time: 0.100, data: 0.001) G_GAN: 0.776 G_L1: 2.332 D_real: 0.817 D_fake: 0.513 \n",
      "(epoch: 13, iters: 112, time: 0.270, data: 0.000) G_GAN: 0.900 G_L1: 2.507 D_real: 0.793 D_fake: 0.384 \n",
      "End of epoch 13 / 200 \t Time Taken: 11 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 14, iters: 88, time: 0.092, data: 0.001) G_GAN: 0.816 G_L1: 2.387 D_real: 0.533 D_fake: 0.784 \n",
      "End of epoch 14 / 200 \t Time Taken: 11 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 15, iters: 64, time: 0.092, data: 0.001) G_GAN: 0.880 G_L1: 2.730 D_real: 0.439 D_fake: 0.648 \n",
      "saving the model at the end of epoch 15, iters 1860\n",
      "End of epoch 15 / 200 \t Time Taken: 13 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 16, iters: 40, time: 0.086, data: 0.002) G_GAN: 0.944 G_L1: 2.508 D_real: 0.663 D_fake: 0.600 \n",
      "End of epoch 16 / 200 \t Time Taken: 11 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 17, iters: 16, time: 0.292, data: 0.001) G_GAN: 0.845 G_L1: 2.273 D_real: 1.244 D_fake: 0.286 \n",
      "(epoch: 17, iters: 116, time: 0.089, data: 0.000) G_GAN: 0.828 G_L1: 1.933 D_real: 0.526 D_fake: 0.765 \n",
      "End of epoch 17 / 200 \t Time Taken: 11 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 18, iters: 92, time: 0.097, data: 0.000) G_GAN: 0.635 G_L1: 2.294 D_real: 1.117 D_fake: 0.400 \n",
      "End of epoch 18 / 200 \t Time Taken: 11 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 19, iters: 68, time: 0.090, data: 0.000) G_GAN: 0.797 G_L1: 2.495 D_real: 0.544 D_fake: 0.641 \n",
      "End of epoch 19 / 200 \t Time Taken: 11 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 20, iters: 44, time: 0.315, data: 0.001) G_GAN: 1.265 G_L1: 2.529 D_real: 0.373 D_fake: 0.911 \n",
      "saving the model at the end of epoch 20, iters 2480\n",
      "End of epoch 20 / 200 \t Time Taken: 12 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 21, iters: 20, time: 0.090, data: 0.001) G_GAN: 0.779 G_L1: 2.678 D_real: 0.779 D_fake: 0.552 \n",
      "(epoch: 21, iters: 120, time: 0.086, data: 0.000) G_GAN: 1.701 G_L1: 2.707 D_real: 0.184 D_fake: 0.995 \n",
      "End of epoch 21 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 22, iters: 96, time: 0.089, data: 0.000) G_GAN: 0.866 G_L1: 1.987 D_real: 0.644 D_fake: 0.652 \n",
      "End of epoch 22 / 200 \t Time Taken: 11 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 23, iters: 72, time: 0.302, data: 0.001) G_GAN: 0.929 G_L1: 2.000 D_real: 0.827 D_fake: 0.452 \n",
      "End of epoch 23 / 200 \t Time Taken: 11 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 24, iters: 48, time: 0.095, data: 0.000) G_GAN: 1.128 G_L1: 1.853 D_real: 0.930 D_fake: 0.382 \n",
      "End of epoch 24 / 200 \t Time Taken: 11 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 25, iters: 24, time: 0.094, data: 0.000) G_GAN: 1.161 G_L1: 1.894 D_real: 0.580 D_fake: 0.565 \n",
      "(epoch: 25, iters: 124, time: 0.097, data: 0.000) G_GAN: 1.024 G_L1: 2.026 D_real: 0.426 D_fake: 0.692 \n",
      "saving the model at the end of epoch 25, iters 3100\n",
      "End of epoch 25 / 200 \t Time Taken: 12 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 26, iters: 100, time: 0.413, data: 3.514) G_GAN: 1.007 G_L1: 1.807 D_real: 0.628 D_fake: 0.412 \n",
      "End of epoch 26 / 200 \t Time Taken: 11 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 27, iters: 76, time: 0.096, data: 0.001) G_GAN: 1.230 G_L1: 2.384 D_real: 0.549 D_fake: 0.465 \n",
      "End of epoch 27 / 200 \t Time Taken: 11 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 28, iters: 52, time: 0.111, data: 0.001) G_GAN: 0.752 G_L1: 2.273 D_real: 0.564 D_fake: 0.713 \n",
      "End of epoch 28 / 200 \t Time Taken: 11 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 29, iters: 28, time: 0.072, data: 0.001) G_GAN: 0.845 G_L1: 1.758 D_real: 0.574 D_fake: 0.776 \n",
      "End of epoch 29 / 200 \t Time Taken: 11 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 30, iters: 4, time: 0.351, data: 0.000) G_GAN: 1.397 G_L1: 1.952 D_real: 0.333 D_fake: 0.730 \n",
      "(epoch: 30, iters: 104, time: 0.091, data: 0.001) G_GAN: 0.936 G_L1: 1.988 D_real: 0.859 D_fake: 0.264 \n",
      "saving the model at the end of epoch 30, iters 3720\n",
      "End of epoch 30 / 200 \t Time Taken: 13 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 31, iters: 80, time: 0.095, data: 0.001) G_GAN: 0.765 G_L1: 2.221 D_real: 1.296 D_fake: 0.291 \n",
      "End of epoch 31 / 200 \t Time Taken: 11 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 32, iters: 56, time: 0.093, data: 0.000) G_GAN: 0.849 G_L1: 2.022 D_real: 0.628 D_fake: 0.531 \n",
      "End of epoch 32 / 200 \t Time Taken: 11 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 33, iters: 32, time: 0.310, data: 0.001) G_GAN: 0.728 G_L1: 1.997 D_real: 0.722 D_fake: 0.615 \n",
      "End of epoch 33 / 200 \t Time Taken: 11 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 34, iters: 8, time: 0.070, data: 0.001) G_GAN: 1.507 G_L1: 2.008 D_real: 0.337 D_fake: 1.217 \n",
      "(epoch: 34, iters: 108, time: 0.066, data: 0.000) G_GAN: 1.962 G_L1: 2.307 D_real: 0.211 D_fake: 1.079 \n",
      "End of epoch 34 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 35, iters: 84, time: 0.070, data: 0.000) G_GAN: 1.749 G_L1: 2.100 D_real: 0.308 D_fake: 0.319 \n",
      "saving the model at the end of epoch 35, iters 4340\n",
      "End of epoch 35 / 200 \t Time Taken: 11 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 36, iters: 60, time: 0.290, data: 0.001) G_GAN: 1.103 G_L1: 2.049 D_real: 0.560 D_fake: 0.571 \n",
      "End of epoch 36 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 37, iters: 36, time: 0.066, data: 0.001) G_GAN: 1.398 G_L1: 1.669 D_real: 0.389 D_fake: 0.260 \n",
      "End of epoch 37 / 200 \t Time Taken: 9 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 38, iters: 12, time: 0.067, data: 0.001) G_GAN: 0.619 G_L1: 1.801 D_real: 0.990 D_fake: 0.257 \n",
      "(epoch: 38, iters: 112, time: 0.067, data: 0.001) G_GAN: 1.569 G_L1: 1.791 D_real: 0.242 D_fake: 0.409 \n",
      "End of epoch 38 / 200 \t Time Taken: 9 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 39, iters: 88, time: 0.596, data: 0.001) G_GAN: 1.537 G_L1: 1.793 D_real: 0.335 D_fake: 0.288 \n",
      "End of epoch 39 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 40, iters: 64, time: 0.066, data: 0.000) G_GAN: 1.241 G_L1: 2.068 D_real: 0.875 D_fake: 0.364 \n",
      "saving the model at the end of epoch 40, iters 4960\n",
      "End of epoch 40 / 200 \t Time Taken: 11 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 41, iters: 40, time: 0.075, data: 0.001) G_GAN: 1.990 G_L1: 2.034 D_real: 0.311 D_fake: 0.745 \n",
      "saving the latest model (epoch 41, total_iters 5000)\n",
      "End of epoch 41 / 200 \t Time Taken: 12 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 42, iters: 16, time: 0.067, data: 0.001) G_GAN: 0.923 G_L1: 2.166 D_real: 1.041 D_fake: 0.436 \n",
      "(epoch: 42, iters: 116, time: 0.314, data: 0.001) G_GAN: 1.475 G_L1: 2.369 D_real: 0.527 D_fake: 0.306 \n",
      "End of epoch 42 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 43, iters: 92, time: 0.096, data: 0.001) G_GAN: 2.164 G_L1: 2.232 D_real: 0.306 D_fake: 0.418 \n",
      "End of epoch 43 / 200 \t Time Taken: 11 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 44, iters: 68, time: 0.067, data: 0.000) G_GAN: 1.434 G_L1: 1.626 D_real: 0.242 D_fake: 0.623 \n",
      "End of epoch 44 / 200 \t Time Taken: 11 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 45, iters: 44, time: 0.067, data: 0.000) G_GAN: 1.990 G_L1: 2.355 D_real: 0.416 D_fake: 0.771 \n",
      "saving the model at the end of epoch 45, iters 5580\n",
      "End of epoch 45 / 200 \t Time Taken: 11 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 46, iters: 20, time: 0.361, data: 0.000) G_GAN: 1.894 G_L1: 1.679 D_real: 0.335 D_fake: 0.268 \n",
      "(epoch: 46, iters: 120, time: 0.072, data: 0.000) G_GAN: 1.368 G_L1: 1.880 D_real: 0.837 D_fake: 0.684 \n",
      "End of epoch 46 / 200 \t Time Taken: 11 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 47, iters: 96, time: 0.071, data: 0.001) G_GAN: 2.220 G_L1: 1.926 D_real: 0.345 D_fake: 0.065 \n",
      "End of epoch 47 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 48, iters: 72, time: 0.066, data: 0.000) G_GAN: 2.667 G_L1: 2.137 D_real: 0.174 D_fake: 0.103 \n",
      "End of epoch 48 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 49, iters: 48, time: 0.310, data: 0.000) G_GAN: 1.465 G_L1: 2.030 D_real: 0.127 D_fake: 1.224 \n",
      "End of epoch 49 / 200 \t Time Taken: 9 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 50, iters: 24, time: 0.066, data: 0.001) G_GAN: 0.755 G_L1: 1.474 D_real: 0.972 D_fake: 0.643 \n",
      "(epoch: 50, iters: 124, time: 0.066, data: 0.001) G_GAN: 2.231 G_L1: 2.025 D_real: 0.513 D_fake: 0.430 \n",
      "saving the model at the end of epoch 50, iters 6200\n",
      "End of epoch 50 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 51, iters: 100, time: 0.067, data: 3.367) G_GAN: 1.837 G_L1: 2.176 D_real: 0.156 D_fake: 0.307 \n",
      "End of epoch 51 / 200 \t Time Taken: 9 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 52, iters: 76, time: 0.327, data: 0.001) G_GAN: 2.740 G_L1: 2.569 D_real: 0.036 D_fake: 0.830 \n",
      "End of epoch 52 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 53, iters: 52, time: 0.067, data: 0.000) G_GAN: 3.066 G_L1: 1.889 D_real: 0.325 D_fake: 0.602 \n",
      "End of epoch 53 / 200 \t Time Taken: 9 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 54, iters: 28, time: 0.067, data: 0.000) G_GAN: 3.328 G_L1: 1.920 D_real: 0.722 D_fake: 0.494 \n",
      "End of epoch 54 / 200 \t Time Taken: 9 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 55, iters: 4, time: 0.052, data: 0.000) G_GAN: 2.055 G_L1: 1.732 D_real: 0.165 D_fake: 0.391 \n",
      "(epoch: 55, iters: 104, time: 0.327, data: 0.000) G_GAN: 2.556 G_L1: 2.024 D_real: 0.347 D_fake: 0.028 \n",
      "saving the model at the end of epoch 55, iters 6820\n",
      "End of epoch 55 / 200 \t Time Taken: 11 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 56, iters: 80, time: 0.080, data: 0.000) G_GAN: 3.702 G_L1: 1.912 D_real: 0.041 D_fake: 0.485 \n",
      "End of epoch 56 / 200 \t Time Taken: 9 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 57, iters: 56, time: 0.067, data: 0.001) G_GAN: 2.834 G_L1: 2.301 D_real: 0.086 D_fake: 0.599 \n",
      "End of epoch 57 / 200 \t Time Taken: 9 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 58, iters: 32, time: 0.074, data: 0.001) G_GAN: 1.696 G_L1: 1.868 D_real: 1.147 D_fake: 0.212 \n",
      "End of epoch 58 / 200 \t Time Taken: 9 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 59, iters: 8, time: 0.341, data: 0.000) G_GAN: 3.044 G_L1: 1.466 D_real: 0.231 D_fake: 0.507 \n",
      "(epoch: 59, iters: 108, time: 0.070, data: 0.001) G_GAN: 1.961 G_L1: 1.712 D_real: 0.246 D_fake: 0.196 \n",
      "End of epoch 59 / 200 \t Time Taken: 9 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 60, iters: 84, time: 0.071, data: 0.000) G_GAN: 3.348 G_L1: 1.932 D_real: 0.089 D_fake: 0.370 \n",
      "saving the model at the end of epoch 60, iters 7440\n",
      "End of epoch 60 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 61, iters: 60, time: 0.069, data: 0.001) G_GAN: 1.924 G_L1: 1.822 D_real: 0.864 D_fake: 0.046 \n",
      "End of epoch 61 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 62, iters: 36, time: 0.377, data: 0.000) G_GAN: 1.467 G_L1: 1.738 D_real: 0.444 D_fake: 0.237 \n",
      "End of epoch 62 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 63, iters: 12, time: 0.067, data: 0.000) G_GAN: 3.662 G_L1: 1.875 D_real: 0.126 D_fake: 0.866 \n",
      "(epoch: 63, iters: 112, time: 0.072, data: 0.001) G_GAN: 4.022 G_L1: 2.131 D_real: 0.627 D_fake: 0.013 \n",
      "End of epoch 63 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 64, iters: 88, time: 0.067, data: 0.001) G_GAN: 1.386 G_L1: 1.604 D_real: 0.548 D_fake: 0.616 \n",
      "End of epoch 64 / 200 \t Time Taken: 9 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 65, iters: 64, time: 0.376, data: 0.001) G_GAN: 2.872 G_L1: 1.838 D_real: 0.038 D_fake: 0.729 \n",
      "saving the model at the end of epoch 65, iters 8060\n",
      "End of epoch 65 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 66, iters: 40, time: 0.068, data: 0.001) G_GAN: 4.960 G_L1: 1.657 D_real: 0.076 D_fake: 1.182 \n",
      "End of epoch 66 / 200 \t Time Taken: 9 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 67, iters: 16, time: 0.066, data: 0.000) G_GAN: 3.339 G_L1: 1.993 D_real: 0.193 D_fake: 0.041 \n",
      "(epoch: 67, iters: 116, time: 0.067, data: 0.001) G_GAN: 2.657 G_L1: 2.510 D_real: 0.297 D_fake: 0.049 \n",
      "End of epoch 67 / 200 \t Time Taken: 9 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 68, iters: 92, time: 0.384, data: 0.001) G_GAN: 2.208 G_L1: 1.951 D_real: 0.566 D_fake: 0.089 \n",
      "End of epoch 68 / 200 \t Time Taken: 9 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 69, iters: 68, time: 0.067, data: 0.001) G_GAN: 0.977 G_L1: 1.792 D_real: 0.878 D_fake: 0.338 \n",
      "End of epoch 69 / 200 \t Time Taken: 9 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 70, iters: 44, time: 0.067, data: 0.000) G_GAN: 0.242 G_L1: 1.779 D_real: 1.282 D_fake: 0.400 \n",
      "saving the model at the end of epoch 70, iters 8680\n",
      "End of epoch 70 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 71, iters: 20, time: 0.068, data: 0.001) G_GAN: 4.198 G_L1: 1.886 D_real: 0.133 D_fake: 0.658 \n",
      "(epoch: 71, iters: 120, time: 0.389, data: 0.001) G_GAN: 2.775 G_L1: 1.776 D_real: 0.099 D_fake: 0.140 \n",
      "End of epoch 71 / 200 \t Time Taken: 9 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 72, iters: 96, time: 0.067, data: 0.001) G_GAN: 3.848 G_L1: 2.212 D_real: 0.087 D_fake: 0.032 \n",
      "End of epoch 72 / 200 \t Time Taken: 9 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 73, iters: 72, time: 0.068, data: 0.001) G_GAN: 2.699 G_L1: 1.747 D_real: 0.168 D_fake: 0.071 \n",
      "End of epoch 73 / 200 \t Time Taken: 9 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 74, iters: 48, time: 0.067, data: 0.000) G_GAN: 2.141 G_L1: 1.903 D_real: 0.152 D_fake: 0.391 \n",
      "End of epoch 74 / 200 \t Time Taken: 9 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 75, iters: 24, time: 0.405, data: 0.000) G_GAN: 2.778 G_L1: 2.172 D_real: 1.154 D_fake: 0.076 \n",
      "(epoch: 75, iters: 124, time: 0.097, data: 0.001) G_GAN: 4.438 G_L1: 1.680 D_real: 0.927 D_fake: 0.024 \n",
      "saving the model at the end of epoch 75, iters 9300\n",
      "End of epoch 75 / 200 \t Time Taken: 12 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 76, iters: 100, time: 0.066, data: 3.693) G_GAN: 2.173 G_L1: 1.666 D_real: 0.492 D_fake: 0.089 \n",
      "End of epoch 76 / 200 \t Time Taken: 9 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 77, iters: 76, time: 0.071, data: 0.000) G_GAN: 2.260 G_L1: 2.477 D_real: 0.270 D_fake: 0.212 \n",
      "End of epoch 77 / 200 \t Time Taken: 9 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 78, iters: 52, time: 0.457, data: 0.000) G_GAN: 2.631 G_L1: 1.752 D_real: 0.258 D_fake: 0.149 \n",
      "End of epoch 78 / 200 \t Time Taken: 11 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 79, iters: 28, time: 0.067, data: 0.000) G_GAN: 3.611 G_L1: 1.994 D_real: 0.065 D_fake: 0.075 \n",
      "End of epoch 79 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 80, iters: 4, time: 0.045, data: 0.000) G_GAN: 2.565 G_L1: 1.791 D_real: 0.211 D_fake: 0.146 \n",
      "(epoch: 80, iters: 104, time: 0.069, data: 0.000) G_GAN: 3.113 G_L1: 1.780 D_real: 0.166 D_fake: 0.253 \n",
      "saving the model at the end of epoch 80, iters 9920\n",
      "End of epoch 80 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 81, iters: 80, time: 0.445, data: 0.001) G_GAN: 2.606 G_L1: 1.728 D_real: 0.397 D_fake: 0.579 \n",
      "saving the latest model (epoch 81, total_iters 10000)\n",
      "End of epoch 81 / 200 \t Time Taken: 11 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 82, iters: 56, time: 0.065, data: 0.000) G_GAN: 2.502 G_L1: 1.468 D_real: 0.018 D_fake: 0.792 \n",
      "End of epoch 82 / 200 \t Time Taken: 9 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 83, iters: 32, time: 0.075, data: 0.000) G_GAN: 2.327 G_L1: 1.645 D_real: 0.072 D_fake: 0.250 \n",
      "End of epoch 83 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 84, iters: 8, time: 0.083, data: 0.000) G_GAN: 3.304 G_L1: 1.974 D_real: 1.228 D_fake: 0.011 \n",
      "(epoch: 84, iters: 108, time: 0.389, data: 0.001) G_GAN: 1.728 G_L1: 2.143 D_real: 0.413 D_fake: 0.300 \n",
      "End of epoch 84 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 85, iters: 84, time: 0.067, data: 0.001) G_GAN: 2.931 G_L1: 1.667 D_real: 0.106 D_fake: 1.228 \n",
      "saving the model at the end of epoch 85, iters 10540\n",
      "End of epoch 85 / 200 \t Time Taken: 11 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 86, iters: 60, time: 0.068, data: 0.001) G_GAN: 3.031 G_L1: 1.848 D_real: 0.134 D_fake: 0.476 \n",
      "End of epoch 86 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 87, iters: 36, time: 0.067, data: 0.001) G_GAN: 3.234 G_L1: 2.143 D_real: 0.052 D_fake: 0.317 \n",
      "End of epoch 87 / 200 \t Time Taken: 9 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 88, iters: 12, time: 0.437, data: 0.001) G_GAN: 2.818 G_L1: 1.624 D_real: 0.200 D_fake: 0.120 \n",
      "(epoch: 88, iters: 112, time: 0.067, data: 0.000) G_GAN: 4.114 G_L1: 1.665 D_real: 0.067 D_fake: 0.028 \n",
      "End of epoch 88 / 200 \t Time Taken: 9 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 89, iters: 88, time: 0.067, data: 0.001) G_GAN: 1.900 G_L1: 2.009 D_real: 0.195 D_fake: 0.336 \n",
      "End of epoch 89 / 200 \t Time Taken: 9 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 90, iters: 64, time: 0.070, data: 0.001) G_GAN: 1.781 G_L1: 1.740 D_real: 0.451 D_fake: 0.713 \n",
      "saving the model at the end of epoch 90, iters 11160\n",
      "End of epoch 90 / 200 \t Time Taken: 11 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 91, iters: 40, time: 0.430, data: 0.000) G_GAN: 3.634 G_L1: 2.018 D_real: 0.130 D_fake: 0.063 \n",
      "End of epoch 91 / 200 \t Time Taken: 9 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 92, iters: 16, time: 0.067, data: 0.000) G_GAN: 2.578 G_L1: 1.867 D_real: 0.194 D_fake: 0.093 \n",
      "(epoch: 92, iters: 116, time: 0.067, data: 0.001) G_GAN: 3.691 G_L1: 1.808 D_real: 0.047 D_fake: 0.100 \n",
      "End of epoch 92 / 200 \t Time Taken: 9 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 93, iters: 92, time: 0.068, data: 0.001) G_GAN: 1.647 G_L1: 1.688 D_real: 0.446 D_fake: 0.631 \n",
      "End of epoch 93 / 200 \t Time Taken: 9 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 94, iters: 68, time: 0.419, data: 0.000) G_GAN: 0.675 G_L1: 1.762 D_real: 1.015 D_fake: 0.457 \n",
      "End of epoch 94 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 95, iters: 44, time: 0.068, data: 0.001) G_GAN: 3.141 G_L1: 1.954 D_real: 0.090 D_fake: 0.072 \n",
      "saving the model at the end of epoch 95, iters 11780\n",
      "End of epoch 95 / 200 \t Time Taken: 11 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 96, iters: 20, time: 0.067, data: 0.000) G_GAN: 1.939 G_L1: 1.704 D_real: 0.950 D_fake: 0.099 \n",
      "(epoch: 96, iters: 120, time: 0.066, data: 0.001) G_GAN: 3.456 G_L1: 1.769 D_real: 0.044 D_fake: 0.163 \n",
      "End of epoch 96 / 200 \t Time Taken: 9 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 97, iters: 96, time: 0.446, data: 0.000) G_GAN: 3.605 G_L1: 1.621 D_real: 0.099 D_fake: 0.126 \n",
      "End of epoch 97 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 98, iters: 72, time: 0.068, data: 0.001) G_GAN: 3.208 G_L1: 1.710 D_real: 0.102 D_fake: 0.208 \n",
      "End of epoch 98 / 200 \t Time Taken: 9 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 99, iters: 48, time: 0.067, data: 0.000) G_GAN: 1.841 G_L1: 1.695 D_real: 0.209 D_fake: 0.135 \n",
      "End of epoch 99 / 200 \t Time Taken: 9 sec\n",
      "learning rate 0.0002000 -> 0.0001980\n",
      "(epoch: 100, iters: 24, time: 0.070, data: 0.000) G_GAN: 2.751 G_L1: 1.933 D_real: 0.111 D_fake: 0.130 \n",
      "(epoch: 100, iters: 124, time: 0.443, data: 0.000) G_GAN: 5.654 G_L1: 1.842 D_real: 0.015 D_fake: 0.010 \n",
      "saving the model at the end of epoch 100, iters 12400\n",
      "End of epoch 100 / 200 \t Time Taken: 11 sec\n",
      "learning rate 0.0001980 -> 0.0001960\n",
      "(epoch: 101, iters: 100, time: 0.066, data: 3.313) G_GAN: 3.727 G_L1: 1.990 D_real: 0.030 D_fake: 0.945 \n",
      "End of epoch 101 / 200 \t Time Taken: 9 sec\n",
      "learning rate 0.0001960 -> 0.0001941\n",
      "(epoch: 102, iters: 76, time: 0.067, data: 0.001) G_GAN: 2.368 G_L1: 1.650 D_real: 1.100 D_fake: 0.090 \n",
      "End of epoch 102 / 200 \t Time Taken: 9 sec\n",
      "learning rate 0.0001941 -> 0.0001921\n",
      "(epoch: 103, iters: 52, time: 0.067, data: 0.001) G_GAN: 2.221 G_L1: 1.633 D_real: 1.183 D_fake: 0.126 \n",
      "End of epoch 103 / 200 \t Time Taken: 9 sec\n",
      "learning rate 0.0001921 -> 0.0001901\n",
      "(epoch: 104, iters: 28, time: 0.522, data: 0.001) G_GAN: 3.605 G_L1: 1.953 D_real: 0.016 D_fake: 0.071 \n",
      "End of epoch 104 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0001901 -> 0.0001881\n",
      "(epoch: 105, iters: 4, time: 0.045, data: 0.001) G_GAN: 3.560 G_L1: 1.692 D_real: 0.035 D_fake: 0.190 \n",
      "(epoch: 105, iters: 104, time: 0.067, data: 0.000) G_GAN: 4.058 G_L1: 1.716 D_real: 0.083 D_fake: 0.784 \n",
      "saving the model at the end of epoch 105, iters 13020\n",
      "End of epoch 105 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0001881 -> 0.0001861\n",
      "(epoch: 106, iters: 80, time: 0.071, data: 0.000) G_GAN: 1.872 G_L1: 1.608 D_real: 0.072 D_fake: 1.295 \n",
      "End of epoch 106 / 200 \t Time Taken: 9 sec\n",
      "learning rate 0.0001861 -> 0.0001842\n",
      "(epoch: 107, iters: 56, time: 0.460, data: 0.000) G_GAN: 3.891 G_L1: 2.047 D_real: 0.179 D_fake: 0.070 \n",
      "End of epoch 107 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0001842 -> 0.0001822\n",
      "(epoch: 108, iters: 32, time: 0.067, data: 0.000) G_GAN: 2.929 G_L1: 1.394 D_real: 0.059 D_fake: 0.071 \n",
      "End of epoch 108 / 200 \t Time Taken: 9 sec\n",
      "learning rate 0.0001822 -> 0.0001802\n",
      "(epoch: 109, iters: 8, time: 0.066, data: 0.000) G_GAN: 3.868 G_L1: 2.015 D_real: 0.019 D_fake: 0.030 \n",
      "(epoch: 109, iters: 108, time: 0.066, data: 0.000) G_GAN: 7.605 G_L1: 2.366 D_real: 0.030 D_fake: 0.002 \n",
      "End of epoch 109 / 200 \t Time Taken: 9 sec\n",
      "learning rate 0.0001802 -> 0.0001782\n",
      "(epoch: 110, iters: 84, time: 0.474, data: 0.001) G_GAN: 4.179 G_L1: 1.496 D_real: 0.358 D_fake: 0.258 \n",
      "saving the model at the end of epoch 110, iters 13640\n",
      "End of epoch 110 / 200 \t Time Taken: 11 sec\n",
      "learning rate 0.0001782 -> 0.0001762\n",
      "(epoch: 111, iters: 60, time: 0.067, data: 0.000) G_GAN: 4.334 G_L1: 1.655 D_real: 0.041 D_fake: 0.027 \n",
      "End of epoch 111 / 200 \t Time Taken: 9 sec\n",
      "learning rate 0.0001762 -> 0.0001743\n",
      "(epoch: 112, iters: 36, time: 0.067, data: 0.000) G_GAN: 2.965 G_L1: 1.735 D_real: 0.356 D_fake: 0.098 \n",
      "End of epoch 112 / 200 \t Time Taken: 9 sec\n",
      "learning rate 0.0001743 -> 0.0001723\n",
      "(epoch: 113, iters: 12, time: 0.066, data: 0.000) G_GAN: 0.916 G_L1: 1.662 D_real: 0.326 D_fake: 1.786 \n",
      "(epoch: 113, iters: 112, time: 0.463, data: 0.001) G_GAN: 4.734 G_L1: 1.566 D_real: 0.560 D_fake: 0.014 \n",
      "End of epoch 113 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0001723 -> 0.0001703\n",
      "(epoch: 114, iters: 88, time: 0.066, data: 0.001) G_GAN: 3.357 G_L1: 1.623 D_real: 0.180 D_fake: 0.349 \n",
      "End of epoch 114 / 200 \t Time Taken: 9 sec\n",
      "learning rate 0.0001703 -> 0.0001683\n",
      "(epoch: 115, iters: 64, time: 0.068, data: 0.001) G_GAN: 6.961 G_L1: 1.518 D_real: 0.098 D_fake: 0.003 \n",
      "saving the model at the end of epoch 115, iters 14260\n",
      "End of epoch 115 / 200 \t Time Taken: 11 sec\n",
      "learning rate 0.0001683 -> 0.0001663\n",
      "(epoch: 116, iters: 40, time: 0.071, data: 0.001) G_GAN: 1.963 G_L1: 1.613 D_real: 0.761 D_fake: 0.341 \n",
      "End of epoch 116 / 200 \t Time Taken: 9 sec\n",
      "learning rate 0.0001663 -> 0.0001644\n",
      "(epoch: 117, iters: 16, time: 0.472, data: 0.001) G_GAN: 3.193 G_L1: 1.712 D_real: 0.093 D_fake: 0.442 \n",
      "(epoch: 117, iters: 116, time: 0.066, data: 0.001) G_GAN: 3.451 G_L1: 1.768 D_real: 0.067 D_fake: 0.291 \n",
      "End of epoch 117 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0001644 -> 0.0001624\n",
      "(epoch: 118, iters: 92, time: 0.071, data: 0.001) G_GAN: 3.376 G_L1: 1.685 D_real: 0.504 D_fake: 0.327 \n",
      "End of epoch 118 / 200 \t Time Taken: 12 sec\n",
      "learning rate 0.0001624 -> 0.0001604\n",
      "(epoch: 119, iters: 68, time: 0.085, data: 0.001) G_GAN: 3.035 G_L1: 1.754 D_real: 0.017 D_fake: 0.162 \n",
      "End of epoch 119 / 200 \t Time Taken: 12 sec\n",
      "learning rate 0.0001604 -> 0.0001584\n",
      "(epoch: 120, iters: 44, time: 0.520, data: 0.000) G_GAN: 2.043 G_L1: 1.647 D_real: 1.342 D_fake: 0.052 \n",
      "saving the model at the end of epoch 120, iters 14880\n",
      "End of epoch 120 / 200 \t Time Taken: 13 sec\n",
      "learning rate 0.0001584 -> 0.0001564\n",
      "(epoch: 121, iters: 20, time: 0.071, data: 0.001) G_GAN: 3.318 G_L1: 1.817 D_real: 0.111 D_fake: 0.094 \n",
      "(epoch: 121, iters: 120, time: 0.067, data: 0.000) G_GAN: 5.204 G_L1: 1.682 D_real: 0.715 D_fake: 0.002 \n",
      "saving the latest model (epoch 121, total_iters 15000)\n",
      "End of epoch 121 / 200 \t Time Taken: 11 sec\n",
      "learning rate 0.0001564 -> 0.0001545\n",
      "(epoch: 122, iters: 96, time: 0.098, data: 0.000) G_GAN: 3.553 G_L1: 1.418 D_real: 0.084 D_fake: 0.042 \n",
      "End of epoch 122 / 200 \t Time Taken: 11 sec\n",
      "learning rate 0.0001545 -> 0.0001525\n",
      "(epoch: 123, iters: 72, time: 0.583, data: 0.001) G_GAN: 4.260 G_L1: 1.667 D_real: 0.009 D_fake: 0.042 \n",
      "End of epoch 123 / 200 \t Time Taken: 11 sec\n",
      "learning rate 0.0001525 -> 0.0001505\n",
      "(epoch: 124, iters: 48, time: 0.067, data: 0.000) G_GAN: 2.853 G_L1: 1.708 D_real: 0.146 D_fake: 0.246 \n",
      "End of epoch 124 / 200 \t Time Taken: 9 sec\n",
      "learning rate 0.0001505 -> 0.0001485\n",
      "(epoch: 125, iters: 24, time: 0.067, data: 0.000) G_GAN: 4.043 G_L1: 1.831 D_real: 0.220 D_fake: 0.028 \n",
      "(epoch: 125, iters: 124, time: 0.066, data: 0.001) G_GAN: 5.595 G_L1: 1.764 D_real: 2.141 D_fake: 0.005 \n",
      "saving the model at the end of epoch 125, iters 15500\n",
      "End of epoch 125 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0001485 -> 0.0001465\n",
      "(epoch: 126, iters: 100, time: 0.514, data: 3.675) G_GAN: 2.950 G_L1: 1.652 D_real: 0.181 D_fake: 0.365 \n",
      "End of epoch 126 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0001465 -> 0.0001446\n",
      "(epoch: 127, iters: 76, time: 0.067, data: 0.000) G_GAN: 3.389 G_L1: 1.844 D_real: 0.166 D_fake: 0.093 \n",
      "End of epoch 127 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0001446 -> 0.0001426\n",
      "(epoch: 128, iters: 52, time: 0.061, data: 0.001) G_GAN: 2.402 G_L1: 1.624 D_real: 0.242 D_fake: 0.373 \n",
      "End of epoch 128 / 200 \t Time Taken: 9 sec\n",
      "learning rate 0.0001426 -> 0.0001406\n",
      "(epoch: 129, iters: 28, time: 0.072, data: 0.001) G_GAN: 4.215 G_L1: 1.641 D_real: 0.007 D_fake: 0.778 \n",
      "End of epoch 129 / 200 \t Time Taken: 9 sec\n",
      "learning rate 0.0001406 -> 0.0001386\n",
      "(epoch: 130, iters: 4, time: 0.599, data: 0.001) G_GAN: 5.790 G_L1: 1.499 D_real: 0.220 D_fake: 0.007 \n",
      "(epoch: 130, iters: 104, time: 0.075, data: 0.001) G_GAN: 3.309 G_L1: 1.517 D_real: 0.097 D_fake: 0.067 \n",
      "saving the model at the end of epoch 130, iters 16120\n",
      "End of epoch 130 / 200 \t Time Taken: 11 sec\n",
      "learning rate 0.0001386 -> 0.0001366\n",
      "(epoch: 131, iters: 80, time: 0.098, data: 0.001) G_GAN: 3.615 G_L1: 1.708 D_real: 0.273 D_fake: 0.063 \n",
      "End of epoch 131 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0001366 -> 0.0001347\n",
      "(epoch: 132, iters: 56, time: 0.068, data: 0.001) G_GAN: 5.593 G_L1: 2.099 D_real: 0.309 D_fake: 0.003 \n",
      "End of epoch 132 / 200 \t Time Taken: 9 sec\n",
      "learning rate 0.0001347 -> 0.0001327\n",
      "(epoch: 133, iters: 32, time: 0.506, data: 0.000) G_GAN: 1.714 G_L1: 1.716 D_real: 0.071 D_fake: 1.051 \n",
      "End of epoch 133 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0001327 -> 0.0001307\n",
      "(epoch: 134, iters: 8, time: 0.068, data: 0.000) G_GAN: 4.156 G_L1: 1.962 D_real: 0.013 D_fake: 0.047 \n",
      "(epoch: 134, iters: 108, time: 0.066, data: 0.000) G_GAN: 8.539 G_L1: 1.696 D_real: 0.029 D_fake: 0.001 \n",
      "End of epoch 134 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0001307 -> 0.0001287\n",
      "(epoch: 135, iters: 84, time: 0.106, data: 0.000) G_GAN: 0.963 G_L1: 1.350 D_real: 0.519 D_fake: 0.990 \n",
      "saving the model at the end of epoch 135, iters 16740\n",
      "End of epoch 135 / 200 \t Time Taken: 11 sec\n",
      "learning rate 0.0001287 -> 0.0001267\n",
      "(epoch: 136, iters: 60, time: 0.480, data: 0.001) G_GAN: 0.955 G_L1: 1.771 D_real: 0.741 D_fake: 0.392 \n",
      "End of epoch 136 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0001267 -> 0.0001248\n",
      "(epoch: 137, iters: 36, time: 0.067, data: 0.000) G_GAN: 1.105 G_L1: 1.535 D_real: 0.540 D_fake: 0.497 \n",
      "End of epoch 137 / 200 \t Time Taken: 9 sec\n",
      "learning rate 0.0001248 -> 0.0001228\n",
      "(epoch: 138, iters: 12, time: 0.100, data: 0.000) G_GAN: 1.380 G_L1: 1.452 D_real: 0.522 D_fake: 0.794 \n",
      "(epoch: 138, iters: 112, time: 0.068, data: 0.001) G_GAN: 1.618 G_L1: 1.708 D_real: 0.502 D_fake: 0.245 \n",
      "End of epoch 138 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0001228 -> 0.0001208\n",
      "(epoch: 139, iters: 88, time: 0.488, data: 0.001) G_GAN: 2.614 G_L1: 2.043 D_real: 0.035 D_fake: 0.225 \n",
      "End of epoch 139 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0001208 -> 0.0001188\n",
      "(epoch: 140, iters: 64, time: 0.067, data: 0.001) G_GAN: 3.347 G_L1: 1.475 D_real: 0.026 D_fake: 0.067 \n",
      "saving the model at the end of epoch 140, iters 17360\n",
      "End of epoch 140 / 200 \t Time Taken: 11 sec\n",
      "learning rate 0.0001188 -> 0.0001168\n",
      "(epoch: 141, iters: 40, time: 0.066, data: 0.001) G_GAN: 3.446 G_L1: 1.768 D_real: 0.257 D_fake: 0.461 \n",
      "End of epoch 141 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0001168 -> 0.0001149\n",
      "(epoch: 142, iters: 16, time: 0.098, data: 0.000) G_GAN: 3.426 G_L1: 1.707 D_real: 0.010 D_fake: 0.232 \n",
      "(epoch: 142, iters: 116, time: 0.549, data: 0.001) G_GAN: 2.498 G_L1: 1.470 D_real: 0.213 D_fake: 0.223 \n",
      "End of epoch 142 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0001149 -> 0.0001129\n",
      "(epoch: 143, iters: 92, time: 0.067, data: 0.000) G_GAN: 2.985 G_L1: 1.623 D_real: 0.140 D_fake: 0.125 \n",
      "End of epoch 143 / 200 \t Time Taken: 9 sec\n",
      "learning rate 0.0001129 -> 0.0001109\n",
      "(epoch: 144, iters: 68, time: 0.067, data: 0.000) G_GAN: 3.401 G_L1: 1.868 D_real: 0.228 D_fake: 0.047 \n",
      "End of epoch 144 / 200 \t Time Taken: 9 sec\n",
      "learning rate 0.0001109 -> 0.0001089\n",
      "(epoch: 145, iters: 44, time: 0.066, data: 0.000) G_GAN: 3.354 G_L1: 1.579 D_real: 0.208 D_fake: 0.163 \n",
      "saving the model at the end of epoch 145, iters 17980\n",
      "End of epoch 145 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0001089 -> 0.0001069\n",
      "(epoch: 146, iters: 20, time: 0.487, data: 0.000) G_GAN: 2.029 G_L1: 1.724 D_real: 0.269 D_fake: 0.411 \n",
      "(epoch: 146, iters: 120, time: 0.066, data: 0.001) G_GAN: 8.763 G_L1: 1.980 D_real: 0.137 D_fake: 0.001 \n",
      "End of epoch 146 / 200 \t Time Taken: 9 sec\n",
      "learning rate 0.0001069 -> 0.0001050\n",
      "(epoch: 147, iters: 96, time: 0.068, data: 0.001) G_GAN: 5.454 G_L1: 1.753 D_real: 0.071 D_fake: 0.011 \n",
      "End of epoch 147 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0001050 -> 0.0001030\n",
      "(epoch: 148, iters: 72, time: 0.071, data: 0.000) G_GAN: 2.589 G_L1: 1.953 D_real: 0.187 D_fake: 0.370 \n",
      "End of epoch 148 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0001030 -> 0.0001010\n",
      "(epoch: 149, iters: 48, time: 0.484, data: 0.001) G_GAN: 1.374 G_L1: 1.688 D_real: 1.929 D_fake: 0.302 \n",
      "End of epoch 149 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0001010 -> 0.0000990\n",
      "(epoch: 150, iters: 24, time: 0.066, data: 0.001) G_GAN: 4.093 G_L1: 1.479 D_real: 0.072 D_fake: 0.135 \n",
      "(epoch: 150, iters: 124, time: 0.063, data: 0.001) G_GAN: 2.793 G_L1: 1.603 D_real: 0.007 D_fake: 0.223 \n",
      "saving the model at the end of epoch 150, iters 18600\n",
      "End of epoch 150 / 200 \t Time Taken: 11 sec\n",
      "learning rate 0.0000990 -> 0.0000970\n",
      "(epoch: 151, iters: 100, time: 0.074, data: 4.396) G_GAN: 4.169 G_L1: 1.905 D_real: 0.092 D_fake: 0.035 \n",
      "End of epoch 151 / 200 \t Time Taken: 12 sec\n",
      "learning rate 0.0000970 -> 0.0000950\n",
      "(epoch: 152, iters: 76, time: 0.579, data: 0.001) G_GAN: 2.931 G_L1: 1.616 D_real: 0.411 D_fake: 0.047 \n",
      "End of epoch 152 / 200 \t Time Taken: 11 sec\n",
      "learning rate 0.0000950 -> 0.0000931\n",
      "(epoch: 153, iters: 52, time: 0.079, data: 0.000) G_GAN: 4.339 G_L1: 1.401 D_real: 0.012 D_fake: 2.073 \n",
      "End of epoch 153 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0000931 -> 0.0000911\n",
      "(epoch: 154, iters: 28, time: 0.067, data: 0.001) G_GAN: 5.644 G_L1: 1.956 D_real: 0.596 D_fake: 0.005 \n",
      "End of epoch 154 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0000911 -> 0.0000891\n",
      "(epoch: 155, iters: 4, time: 0.088, data: 0.001) G_GAN: 3.567 G_L1: 1.398 D_real: 0.039 D_fake: 0.856 \n",
      "(epoch: 155, iters: 104, time: 0.537, data: 0.000) G_GAN: 6.858 G_L1: 1.456 D_real: 0.037 D_fake: 0.003 \n",
      "saving the model at the end of epoch 155, iters 19220\n",
      "End of epoch 155 / 200 \t Time Taken: 11 sec\n",
      "learning rate 0.0000891 -> 0.0000871\n",
      "(epoch: 156, iters: 80, time: 0.068, data: 0.001) G_GAN: 2.530 G_L1: 1.723 D_real: 0.125 D_fake: 0.179 \n",
      "End of epoch 156 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0000871 -> 0.0000851\n",
      "(epoch: 157, iters: 56, time: 0.066, data: 0.000) G_GAN: 3.180 G_L1: 1.816 D_real: 0.123 D_fake: 0.084 \n",
      "End of epoch 157 / 200 \t Time Taken: 11 sec\n",
      "learning rate 0.0000851 -> 0.0000832\n",
      "(epoch: 158, iters: 32, time: 0.066, data: 0.001) G_GAN: 4.400 G_L1: 1.599 D_real: 0.023 D_fake: 0.019 \n",
      "End of epoch 158 / 200 \t Time Taken: 11 sec\n",
      "learning rate 0.0000832 -> 0.0000812\n",
      "(epoch: 159, iters: 8, time: 0.659, data: 0.001) G_GAN: 3.117 G_L1: 1.830 D_real: 0.052 D_fake: 0.407 \n",
      "(epoch: 159, iters: 108, time: 0.070, data: 0.001) G_GAN: 2.407 G_L1: 1.520 D_real: 0.114 D_fake: 0.175 \n",
      "End of epoch 159 / 200 \t Time Taken: 11 sec\n",
      "learning rate 0.0000812 -> 0.0000792\n",
      "(epoch: 160, iters: 84, time: 0.066, data: 0.000) G_GAN: 3.441 G_L1: 1.485 D_real: 0.156 D_fake: 0.347 \n",
      "saving the model at the end of epoch 160, iters 19840\n",
      "End of epoch 160 / 200 \t Time Taken: 11 sec\n",
      "learning rate 0.0000792 -> 0.0000772\n",
      "(epoch: 161, iters: 60, time: 0.067, data: 0.000) G_GAN: 2.081 G_L1: 1.458 D_real: 0.441 D_fake: 0.163 \n",
      "End of epoch 161 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0000772 -> 0.0000752\n",
      "(epoch: 162, iters: 36, time: 0.511, data: 0.000) G_GAN: 2.441 G_L1: 1.767 D_real: 0.199 D_fake: 0.084 \n",
      "saving the latest model (epoch 162, total_iters 20000)\n",
      "End of epoch 162 / 200 \t Time Taken: 11 sec\n",
      "learning rate 0.0000752 -> 0.0000733\n",
      "(epoch: 163, iters: 12, time: 0.090, data: 0.000) G_GAN: 2.312 G_L1: 1.749 D_real: 0.098 D_fake: 0.366 \n",
      "(epoch: 163, iters: 112, time: 0.066, data: 0.001) G_GAN: 1.973 G_L1: 1.779 D_real: 0.026 D_fake: 0.521 \n",
      "End of epoch 163 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0000733 -> 0.0000713\n",
      "(epoch: 164, iters: 88, time: 0.067, data: 0.001) G_GAN: 3.611 G_L1: 1.585 D_real: 0.786 D_fake: 0.018 \n",
      "End of epoch 164 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0000713 -> 0.0000693\n",
      "(epoch: 165, iters: 64, time: 0.558, data: 0.001) G_GAN: 4.054 G_L1: 1.524 D_real: 0.162 D_fake: 0.034 \n",
      "saving the model at the end of epoch 165, iters 20460\n",
      "End of epoch 165 / 200 \t Time Taken: 12 sec\n",
      "learning rate 0.0000693 -> 0.0000673\n",
      "(epoch: 166, iters: 40, time: 0.066, data: 0.001) G_GAN: 2.401 G_L1: 1.656 D_real: 0.458 D_fake: 0.055 \n",
      "End of epoch 166 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0000673 -> 0.0000653\n",
      "(epoch: 167, iters: 16, time: 0.054, data: 0.001) G_GAN: 2.259 G_L1: 1.457 D_real: 0.089 D_fake: 0.421 \n",
      "(epoch: 167, iters: 116, time: 0.066, data: 0.000) G_GAN: 5.098 G_L1: 1.806 D_real: 0.008 D_fake: 0.024 \n",
      "End of epoch 167 / 200 \t Time Taken: 11 sec\n",
      "learning rate 0.0000653 -> 0.0000634\n",
      "(epoch: 168, iters: 92, time: 0.530, data: 0.001) G_GAN: 4.844 G_L1: 1.823 D_real: 1.028 D_fake: 0.023 \n",
      "End of epoch 168 / 200 \t Time Taken: 12 sec\n",
      "learning rate 0.0000634 -> 0.0000614\n",
      "(epoch: 169, iters: 68, time: 0.069, data: 0.001) G_GAN: 1.559 G_L1: 1.922 D_real: 0.185 D_fake: 0.233 \n",
      "End of epoch 169 / 200 \t Time Taken: 9 sec\n",
      "learning rate 0.0000614 -> 0.0000594\n",
      "(epoch: 170, iters: 44, time: 0.070, data: 0.001) G_GAN: 5.150 G_L1: 1.422 D_real: 0.207 D_fake: 0.008 \n",
      "saving the model at the end of epoch 170, iters 21080\n",
      "End of epoch 170 / 200 \t Time Taken: 11 sec\n",
      "learning rate 0.0000594 -> 0.0000574\n",
      "(epoch: 171, iters: 20, time: 0.066, data: 0.001) G_GAN: 2.343 G_L1: 1.716 D_real: 0.083 D_fake: 0.187 \n",
      "(epoch: 171, iters: 120, time: 0.625, data: 0.001) G_GAN: 2.460 G_L1: 1.596 D_real: 0.130 D_fake: 0.165 \n",
      "End of epoch 171 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0000574 -> 0.0000554\n",
      "(epoch: 172, iters: 96, time: 0.066, data: 0.001) G_GAN: 4.030 G_L1: 1.494 D_real: 0.264 D_fake: 0.026 \n",
      "End of epoch 172 / 200 \t Time Taken: 9 sec\n",
      "learning rate 0.0000554 -> 0.0000535\n",
      "(epoch: 173, iters: 72, time: 0.067, data: 0.000) G_GAN: 1.951 G_L1: 1.601 D_real: 0.102 D_fake: 0.446 \n",
      "End of epoch 173 / 200 \t Time Taken: 9 sec\n",
      "learning rate 0.0000535 -> 0.0000515\n",
      "(epoch: 174, iters: 48, time: 0.079, data: 0.001) G_GAN: 2.865 G_L1: 1.941 D_real: 0.149 D_fake: 0.068 \n",
      "End of epoch 174 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0000515 -> 0.0000495\n",
      "(epoch: 175, iters: 24, time: 0.588, data: 0.001) G_GAN: 2.329 G_L1: 1.748 D_real: 0.023 D_fake: 0.143 \n",
      "(epoch: 175, iters: 124, time: 0.065, data: 0.001) G_GAN: 2.599 G_L1: 1.280 D_real: 0.434 D_fake: 0.149 \n",
      "saving the model at the end of epoch 175, iters 21700\n",
      "End of epoch 175 / 200 \t Time Taken: 12 sec\n",
      "learning rate 0.0000495 -> 0.0000475\n",
      "(epoch: 176, iters: 100, time: 0.071, data: 4.087) G_GAN: 1.615 G_L1: 1.493 D_real: 0.468 D_fake: 0.935 \n",
      "End of epoch 176 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0000475 -> 0.0000455\n",
      "(epoch: 177, iters: 76, time: 0.068, data: 0.001) G_GAN: 3.168 G_L1: 1.754 D_real: 0.106 D_fake: 0.116 \n",
      "End of epoch 177 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0000455 -> 0.0000436\n",
      "(epoch: 178, iters: 52, time: 0.703, data: 0.001) G_GAN: 3.843 G_L1: 1.676 D_real: 1.017 D_fake: 0.026 \n",
      "End of epoch 178 / 200 \t Time Taken: 11 sec\n",
      "learning rate 0.0000436 -> 0.0000416\n",
      "(epoch: 179, iters: 28, time: 0.066, data: 0.000) G_GAN: 5.945 G_L1: 1.390 D_real: 0.114 D_fake: 0.004 \n",
      "End of epoch 179 / 200 \t Time Taken: 9 sec\n",
      "learning rate 0.0000416 -> 0.0000396\n",
      "(epoch: 180, iters: 4, time: 0.044, data: 0.000) G_GAN: 3.266 G_L1: 1.718 D_real: 0.078 D_fake: 0.087 \n",
      "(epoch: 180, iters: 104, time: 0.070, data: 0.001) G_GAN: 6.695 G_L1: 1.930 D_real: 0.202 D_fake: 0.002 \n",
      "saving the model at the end of epoch 180, iters 22320\n",
      "End of epoch 180 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0000396 -> 0.0000376\n",
      "(epoch: 181, iters: 80, time: 0.584, data: 0.001) G_GAN: 3.547 G_L1: 1.533 D_real: 0.132 D_fake: 0.057 \n",
      "End of epoch 181 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0000376 -> 0.0000356\n",
      "(epoch: 182, iters: 56, time: 0.071, data: 0.001) G_GAN: 2.080 G_L1: 1.699 D_real: 0.855 D_fake: 0.101 \n",
      "End of epoch 182 / 200 \t Time Taken: 9 sec\n",
      "learning rate 0.0000356 -> 0.0000337\n",
      "(epoch: 183, iters: 32, time: 0.065, data: 0.000) G_GAN: 2.936 G_L1: 1.843 D_real: 0.132 D_fake: 0.260 \n",
      "End of epoch 183 / 200 \t Time Taken: 9 sec\n",
      "learning rate 0.0000337 -> 0.0000317\n",
      "(epoch: 184, iters: 8, time: 0.106, data: 0.000) G_GAN: 3.905 G_L1: 1.390 D_real: 0.208 D_fake: 0.037 \n",
      "(epoch: 184, iters: 108, time: 0.582, data: 0.000) G_GAN: 1.590 G_L1: 1.541 D_real: 0.391 D_fake: 0.963 \n",
      "End of epoch 184 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0000317 -> 0.0000297\n",
      "(epoch: 185, iters: 84, time: 0.107, data: 0.001) G_GAN: 2.249 G_L1: 1.664 D_real: 0.035 D_fake: 0.571 \n",
      "saving the model at the end of epoch 185, iters 22940\n",
      "End of epoch 185 / 200 \t Time Taken: 13 sec\n",
      "learning rate 0.0000297 -> 0.0000277\n",
      "(epoch: 186, iters: 60, time: 0.094, data: 0.001) G_GAN: 5.933 G_L1: 2.163 D_real: 0.406 D_fake: 0.004 \n",
      "End of epoch 186 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0000277 -> 0.0000257\n",
      "(epoch: 187, iters: 36, time: 0.066, data: 0.001) G_GAN: 3.084 G_L1: 1.646 D_real: 0.333 D_fake: 0.150 \n",
      "End of epoch 187 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0000257 -> 0.0000238\n",
      "(epoch: 188, iters: 12, time: 0.654, data: 0.000) G_GAN: 2.165 G_L1: 1.345 D_real: 0.070 D_fake: 0.251 \n",
      "(epoch: 188, iters: 112, time: 0.067, data: 0.000) G_GAN: 1.593 G_L1: 1.684 D_real: 0.087 D_fake: 0.756 \n",
      "End of epoch 188 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0000238 -> 0.0000218\n",
      "(epoch: 189, iters: 88, time: 0.066, data: 0.000) G_GAN: 1.774 G_L1: 1.622 D_real: 0.241 D_fake: 0.492 \n",
      "End of epoch 189 / 200 \t Time Taken: 9 sec\n",
      "learning rate 0.0000218 -> 0.0000198\n",
      "(epoch: 190, iters: 64, time: 0.068, data: 0.000) G_GAN: 3.643 G_L1: 1.661 D_real: 0.012 D_fake: 0.054 \n",
      "saving the model at the end of epoch 190, iters 23560\n",
      "End of epoch 190 / 200 \t Time Taken: 11 sec\n",
      "learning rate 0.0000198 -> 0.0000178\n",
      "(epoch: 191, iters: 40, time: 0.605, data: 0.000) G_GAN: 1.323 G_L1: 1.510 D_real: 0.075 D_fake: 0.709 \n",
      "End of epoch 191 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0000178 -> 0.0000158\n",
      "(epoch: 192, iters: 16, time: 0.066, data: 0.001) G_GAN: 2.490 G_L1: 1.779 D_real: 0.094 D_fake: 0.186 \n",
      "(epoch: 192, iters: 116, time: 0.066, data: 0.001) G_GAN: 1.491 G_L1: 1.876 D_real: 0.501 D_fake: 0.299 \n",
      "End of epoch 192 / 200 \t Time Taken: 9 sec\n",
      "learning rate 0.0000158 -> 0.0000139\n",
      "(epoch: 193, iters: 92, time: 0.066, data: 0.000) G_GAN: 2.591 G_L1: 1.968 D_real: 0.267 D_fake: 0.103 \n",
      "End of epoch 193 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0000139 -> 0.0000119\n",
      "(epoch: 194, iters: 68, time: 0.639, data: 0.001) G_GAN: 1.000 G_L1: 1.713 D_real: 0.161 D_fake: 0.753 \n",
      "End of epoch 194 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0000119 -> 0.0000099\n",
      "(epoch: 195, iters: 44, time: 0.067, data: 0.000) G_GAN: 1.004 G_L1: 1.812 D_real: 0.236 D_fake: 0.720 \n",
      "saving the model at the end of epoch 195, iters 24180\n",
      "End of epoch 195 / 200 \t Time Taken: 11 sec\n",
      "learning rate 0.0000099 -> 0.0000079\n",
      "(epoch: 196, iters: 20, time: 0.067, data: 0.000) G_GAN: 3.197 G_L1: 1.921 D_real: 0.217 D_fake: 0.058 \n",
      "(epoch: 196, iters: 120, time: 0.066, data: 0.000) G_GAN: 1.694 G_L1: 1.677 D_real: 0.739 D_fake: 0.253 \n",
      "End of epoch 196 / 200 \t Time Taken: 9 sec\n",
      "learning rate 0.0000079 -> 0.0000059\n",
      "(epoch: 197, iters: 96, time: 0.658, data: 0.000) G_GAN: 5.789 G_L1: 1.641 D_real: 0.082 D_fake: 0.005 \n",
      "End of epoch 197 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0000059 -> 0.0000040\n",
      "(epoch: 198, iters: 72, time: 0.067, data: 0.000) G_GAN: 2.415 G_L1: 1.473 D_real: 0.508 D_fake: 0.137 \n",
      "End of epoch 198 / 200 \t Time Taken: 9 sec\n",
      "learning rate 0.0000040 -> 0.0000020\n",
      "(epoch: 199, iters: 48, time: 0.067, data: 0.001) G_GAN: 3.800 G_L1: 1.741 D_real: 0.085 D_fake: 0.032 \n",
      "End of epoch 199 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0000020 -> 0.0000000\n",
      "(epoch: 200, iters: 24, time: 0.118, data: 0.000) G_GAN: 1.715 G_L1: 2.171 D_real: 0.077 D_fake: 0.283 \n",
      "(epoch: 200, iters: 124, time: 0.687, data: 0.001) G_GAN: 2.755 G_L1: 1.446 D_real: 0.206 D_fake: 0.092 \n",
      "saving the model at the end of epoch 200, iters 24800\n",
      "End of epoch 200 / 200 \t Time Taken: 12 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n",
      "c:\\Users\\chominkyung\\miniconda3\\envs\\jupyter\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    }
   ],
   "source": [
    "!python train.py --dataroot ./datasets/eyeborw_remove --name eyeborw_remove --model pix2pix --direction AtoB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu116\n",
      "Requirement already satisfied: torch==1.12.1+cu116 in c:\\users\\chominkyung\\miniconda3\\envs\\jupyter\\lib\\site-packages (1.12.1+cu116)\n",
      "Collecting torchvision==0.13.1+cu116\n",
      "  Using cached https://download.pytorch.org/whl/cu116/torchvision-0.13.1%2Bcu116-cp38-cp38-win_amd64.whl (2.6 MB)\n",
      "Collecting torchaudio==0.12.1\n",
      "  Using cached https://download.pytorch.org/whl/cu116/torchaudio-0.12.1%2Bcu116-cp38-cp38-win_amd64.whl (1.2 MB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\chominkyung\\miniconda3\\envs\\jupyter\\lib\\site-packages (from torch==1.12.1+cu116) (4.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\chominkyung\\miniconda3\\envs\\jupyter\\lib\\site-packages (from torchvision==0.13.1+cu116) (1.23.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\chominkyung\\miniconda3\\envs\\jupyter\\lib\\site-packages (from torchvision==0.13.1+cu116) (9.2.0)\n",
      "Requirement already satisfied: requests in c:\\users\\chominkyung\\miniconda3\\envs\\jupyter\\lib\\site-packages (from torchvision==0.13.1+cu116) (2.28.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\chominkyung\\miniconda3\\envs\\jupyter\\lib\\site-packages (from requests->torchvision==0.13.1+cu116) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\chominkyung\\miniconda3\\envs\\jupyter\\lib\\site-packages (from requests->torchvision==0.13.1+cu116) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\chominkyung\\miniconda3\\envs\\jupyter\\lib\\site-packages (from requests->torchvision==0.13.1+cu116) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\chominkyung\\miniconda3\\envs\\jupyter\\lib\\site-packages (from requests->torchvision==0.13.1+cu116) (3.4)\n",
      "Installing collected packages: torchvision, torchaudio\n",
      "Successfully installed torchaudio-0.12.1+cu116 torchvision-0.13.1+cu116\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch==1.12.1+cu116 torchvision==0.13.1+cu116 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu116 --user"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Options ---------------\n",
      "             aspect_ratio: 1.0                           \n",
      "               batch_size: 1                             \n",
      "          checkpoints_dir: ./checkpoints                 \n",
      "                crop_size: 256                           \n",
      "                 dataroot: ./datasets/eyebrow_remove     \t[default: None]\n",
      "             dataset_mode: aligned                       \n",
      "                direction: AtoB                          \n",
      "          display_winsize: 256                           \n",
      "                    epoch: latest                        \n",
      "                     eval: False                         \n",
      "                  gpu_ids: 0                             \n",
      "                init_gain: 0.02                          \n",
      "                init_type: normal                        \n",
      "                 input_nc: 3                             \n",
      "                  isTrain: False                         \t[default: None]\n",
      "                load_iter: 0                             \t[default: 0]\n",
      "                load_size: 256                           \n",
      "         max_dataset_size: inf                           \n",
      "                    model: pix2pix                       \t[default: test]\n",
      "               n_layers_D: 3                             \n",
      "                     name: eyebrow_remove                \t[default: experiment_name]\n",
      "                      ndf: 64                            \n",
      "                     netD: basic                         \n",
      "                     netG: unet_256                      \n",
      "                      ngf: 64                            \n",
      "               no_dropout: False                         \n",
      "                  no_flip: False                         \n",
      "                     norm: batch                         \n",
      "                 num_test: 50                            \n",
      "              num_threads: 4                             \n",
      "                output_nc: 3                             \n",
      "                    phase: test                          \n",
      "               preprocess: resize_and_crop               \n",
      "              results_dir: ./results/                    \n",
      "           serial_batches: False                         \n",
      "                   suffix:                               \n",
      "                use_wandb: False                         \n",
      "                  verbose: False                         \n",
      "       wandb_project_name: CycleGAN-and-pix2pix          \n",
      "----------------- End -------------------\n",
      "this is base_options\n",
      "[0]\n",
      "Namespace(aspect_ratio=1.0, batch_size=1, checkpoints_dir='./checkpoints', crop_size=256, dataroot='./datasets/eyebrow_remove', dataset_mode='aligned', direction='AtoB', display_id=-1, display_winsize=256, epoch='latest', eval=False, gpu_ids=[0], init_gain=0.02, init_type='normal', input_nc=3, isTrain=False, load_iter=0, load_size=256, max_dataset_size=inf, model='pix2pix', n_layers_D=3, name='eyebrow_remove', ndf=64, netD='basic', netG='unet_256', ngf=64, no_dropout=False, no_flip=True, norm='batch', num_test=50, num_threads=0, output_nc=3, phase='test', preprocess='resize_and_crop', results_dir='./results/', serial_batches=True, suffix='', use_wandb=False, verbose=False, wandb_project_name='CycleGAN-and-pix2pix')\n",
      "dataset [AlignedDataset] was created\n",
      "initialize network with normal\n",
      "model [Pix2PixModel] was created\n",
      "loading the model from ./checkpoints\\eyebrow_remove\\latest_net_G.pth\n",
      "---------- Networks initialized -------------\n",
      "[Network G] Total number of parameters : 54.414 M\n",
      "-----------------------------------------------\n",
      "creating web directory ./results/eyebrow_remove\\test_latest\n",
      "processing (0000)-th image... ['./datasets/eyebrow_remove\\\\test\\\\115.jpg']\n",
      "processing (0005)-th image... ['./datasets/eyebrow_remove\\\\test\\\\120.jpg']\n",
      "processing (0010)-th image... ['./datasets/eyebrow_remove\\\\test\\\\125.jpg']\n",
      "processing (0015)-th image... ['./datasets/eyebrow_remove\\\\test\\\\130.jpg']\n",
      "processing (0020)-th image... ['./datasets/eyebrow_remove\\\\test\\\\135.jpg']\n",
      "processing (0025)-th image... ['./datasets/eyebrow_remove\\\\test\\\\140.jpg']\n"
     ]
    }
   ],
   "source": [
    "!python test.py --dataroot ./datasets/eyebrow_remove --name eyebrow_remove --model pix2pix --direction AtoB"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
